import datetime
import enum
import threading
import time
import json
import uuid
import typing

import pydantic
import sqlalchemy
import sqlalchemy.types
import sqlmodel
import pandas as pd

import app.db.core as db
import app.features.domain as features
import app.service_renewables_ninja.service as rninja
import app.service_rli_weather.service as rrli
import app.service_offgrid_planner.demand as demand
import app.service_offgrid_planner.grid as grid
import app.service_offgrid_planner.service as offgrid_planner
import app.service_offgrid_planner.supply as supply
import app.service_offgrid_planner.results as project_result
from app.explorations.clustering import (
    Cluster,
    ClusteringParameters,
    ClusteringParametersCreate,
    generate_clusters,
)


class ExplorationStatus(str, enum.Enum):
    """A FINISHED exploration can contain failed simulations (even all of them could have failed).
    You have to check the status of the simulations to know."""

    RUNNING = "RUNNING"
    FINISHED = "FINISHED"
    STOPPED = "STOPPED"


class Exploration(ClusteringParameters, table=True):
    id: pydantic.UUID4 = sqlmodel.Field(default_factory=uuid.uuid4, primary_key=True)

    status: ExplorationStatus = sqlmodel.Field(
        default=ExplorationStatus.RUNNING,
        sa_column=sqlalchemy.Column(sqlalchemy.Enum(ExplorationStatus), nullable=False),
    )

    minigrids_found: int | None = None

    clusters_found: int | None = None

    clusters_found_at: datetime.datetime | None = None

    optimizer_inputs_generated_at: datetime.datetime | None = None

    optimizer_finished_at: datetime.datetime | None = None

    created_at: datetime.datetime = sqlmodel.Field(default_factory=lambda: datetime.datetime.now())


class ProjectStatus(enum.Enum):
    POTENTIAL = "POTENTIAL"
    PROJECT = "PROJECT"
    MONITORING = "MONITORING"


class SimulationStatus(str, enum.Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    OPTIMIZED = "OPTIMIZED"
    PROCESSED = "PROCESSED"
    STOPPED = "STOPPED"
    ERROR = "ERROR"


class SimulationStatusSql(sqlalchemy.types.TypeDecorator[str]):
    impl = sqlalchemy.String
    cache_ok = True

    def process_bind_param(
        self, value: SimulationStatus | str | None, dialect: sqlalchemy.engine.interfaces.Dialect
    ) -> str | None:
        if isinstance(value, SimulationStatus):
            return value.value
        return value

    def process_result_value(
        self, value: str | None, dialect: sqlalchemy.engine.interfaces.Dialect
    ) -> SimulationStatus | None:
        if value is not None:
            return SimulationStatus(value)
        return value


class Simulation(sqlmodel.SQLModel, table=True):
    id: pydantic.UUID4 = sqlmodel.Field(default_factory=uuid.uuid4, primary_key=True)

    exploration_id: pydantic.UUID4 = sqlmodel.Field(foreign_key="exploration.id")

    cluster_id: int
    """External identifier generated by the clustering algorithm."""

    # WARNING: Be careful when updating the following attributes, read
    # https://amercader.net/blog/beware-of-json-fields-in-sqlalchemy/. Recommended action: use deep
    # copy (probably from pydantic), as explained at the end of the article.

    project_input: str | None = sqlmodel.Field(
        sa_column=sqlalchemy.Column(sqlalchemy.Text), default=None
    )

    grid_input: str = sqlmodel.Field(sa_column=sqlalchemy.Column(sqlalchemy.Text))

    grid_results: str | None = sqlmodel.Field(
        sa_column=sqlalchemy.Column(sqlalchemy.Text), default=None
    )

    supply_input: str = sqlmodel.Field(sa_column=sqlalchemy.Column(sqlalchemy.Text))

    supply_results: str | None = sqlmodel.Field(
        sa_column=sqlalchemy.Column(sqlalchemy.Text), default=None
    )

    optimizer_started_at: datetime.datetime | None = None

    optimizer_failed_at: datetime.datetime | None = None

    stopped_at: datetime.datetime | None = None

    created_at: datetime.datetime = sqlmodel.Field(default_factory=lambda: datetime.datetime.now())

    status: SimulationStatus = sqlmodel.Field(
        default=None,
        sa_column=sqlalchemy.Column(
            SimulationStatusSql(),
            sqlalchemy.Computed(
                """
                CASE
                    WHEN optimizer_failed_at IS NOT NULL THEN 'ERROR'
                    WHEN project_input IS NOT NULL THEN 'PROCESSED'
                    WHEN stopped_at IS NOT NULL THEN 'STOPPED'
                    WHEN grid_results IS NOT NULL AND supply_results IS NOT NULL THEN 'OPTIMIZED'
                    WHEN optimizer_started_at IS NOT NULL THEN 'RUNNING'
                    ELSE 'PENDING'
                END
                """,
                persisted=True,
            ),
            nullable=False,
        ),
    )

    __table_args__ = (
        sqlalchemy.UniqueConstraint(
            "exploration_id", "cluster_id", name="uc_exploration_id_cluster_id"
        ),
    )


class ExplorationError(str, enum.Enum):
    start_clustering_failed = "The clustering algorithm could not be launched"
    clustering_algorithm_failed = "The clustering algorithm failed"


class CategoryDistribution(sqlmodel.SQLModel, table=True):
    """Store the distribution of different categories in the centroids."""

    id: int = sqlmodel.Field(primary_key=True, default=None)
    households: float
    public_services: float
    enterprises: float
    created_at: datetime.datetime = sqlmodel.Field(default_factory=datetime.datetime.now)


def generate_grid_input(total_annual_demand: float, cluster: Cluster) -> grid.GridInput:
    nodes: grid.NodeAttributes[grid.ConsumerType] = grid.NodeAttributes()
    nodes.distribution_cost = {}

    # TODO: depending on building.building_type, some special nodes could be created.
    # TODO: building.surface is not used, atm
    # TODO: check with RLI if it's convenient to fill up nodes.distance_to_load_center, and how.

    # Add the power house node at the cluster centroid
    nodes.latitude[0] = cluster.geography.coordinates.latitude
    nodes.longitude[0] = cluster.geography.coordinates.longitude
    # TODO: check with RLI if 'manual' is the appropriate value here, in their example they use
    # 'k-means' for the power house.
    nodes.how_added[0] = grid.HowAdded.manual
    nodes.node_type[0] = grid.NodeType.power_house
    nodes.consumer_type[0] = grid.ConsumerType.na
    # Same value as in the example provided by RLI:
    nodes.custom_specification[0] = grid.CustomSpecLiteral.none
    # The RLI example uses NaN here, but this is not supported by the type in their schema:
    # TODO: check with RLI if NaN's are treated differently from None
    nodes.shs_options[0] = None
    nodes.consumer_detail[0] = grid.ConsumerDetail.na
    nodes.is_connected[0] = True  # Same value as in the example provided by RLI
    nodes.distribution_cost[0] = 0.0  # Same value as in the example provided by RLI

    # Add consumer nodes
    for node_id, building in enumerate(cluster.buildings_as_objects, start=1):
        nodes.latitude[node_id] = building.latitude
        nodes.longitude[node_id] = building.longitude
        nodes.how_added[node_id] = grid.HowAdded.automatic
        nodes.node_type[node_id] = grid.NodeType.consumer
        nodes.consumer_type[node_id] = grid.ConsumerType.household
        nodes.custom_specification[node_id] = ""  # Same value as in the example provided by RLI
        # The example provided by RLI uses 0.0, but the type in the schema is int:
        nodes.shs_options[node_id] = 0
        nodes.consumer_detail[node_id] = grid.ConsumerDetail.default
        nodes.is_connected[node_id] = True  # Same value as in the example provided by RLI
        # TODO: same problem as with the NaN for nodes.shs_options for the power house:
        nodes.distribution_cost[node_id] = None

    # TODO: check the hypotesis embodied in the numeric arguments to grid_design. Current values
    # have been taken from example file provided by RLI.
    grid_design = grid.GridDesign(
        distribution_cable=grid.DistributionCable(
            lifetime=25, capex=10.0, max_length=50.0, epc=1.3016123200774503
        ),
        connection_cable=grid.ConnectionCable(
            lifetime=25, capex=4.0, max_length=20.0, epc=0.5206449280309802
        ),
        pole=grid.Pole(lifetime=25, capex=800.0, max_n_connections=5, epc=104.12898560619605),
        mg=grid.Mg(connection_cost=140.0, epc=18.222572481084306),
        shs=grid.Shs(include=True, max_grid_cost=0.6),
    )

    return grid.GridInput(nodes=nodes, grid_design=grid_design, yearly_demand=total_annual_demand)


def generate_supply_input(
    hourly_annual_demand: list[float], cluster: Cluster
) -> supply.SupplyInput:
    first_building = cluster.buildings_as_objects[0]

    try:
        solar_potential = rrli.get_pv_data(
            lat=first_building.latitude,
            lon=first_building.longitude,
            dt_index=pd.date_range(
                start="2022-01-01 00:00:00", end="2022-12-31 23:00:00", freq="h"
            ),
        )
    except Exception as e:
        print("Error fetching RLI weather data, falling back to renewables.ninja:", e)
        solar_potential = rninja.get_pv_data(
            lat=first_building.latitude, lon=first_building.longitude
        )

    first_of_year = datetime.datetime(datetime.datetime.now().year, 1, 1, 0, 0, 0)

    # TODO: what happens in years with 366 days?
    index = supply.Index(start_date=first_of_year, n_days=365, freq=supply.Freq.h)
    sequences = supply.Sequences(
        index=index, demand=hourly_annual_demand, solar_potential=solar_potential
    )

    # TODO: check that the settings/parameters for the energy system design, taken from RLI example,
    # are correct.
    energy_system_design = supply.EnergySystemDesign.model_validate(
        {
            "battery": {
                "settings": {"is_selected": True, "design": True},
                "parameters": {
                    "nominal_capacity": None,
                    "lifetime": 7,
                    "capex": 314.0,
                    "opex": 24.0,
                    "soc_min": 0.0,
                    "soc_max": 10.0,
                    "c_rate_in": 1.0,
                    "c_rate_out": 1.0,
                    "efficiency": 0.96,
                    "epc": 94.56299561338449,
                },
            },
            "diesel_genset": {
                "settings": {"is_selected": True, "design": True},
                "parameters": {
                    "nominal_capacity": None,
                    "lifetime": 8,
                    "capex": 350.0,
                    "opex": 25.0,
                    "variable_cost": 0.0,
                    "fuel_cost": 1.7,
                    "fuel_lhv": 11.8,
                    "min_load": 20.0,
                    "max_load": 100.0,
                    "min_efficiency": 0.22,
                    "max_efficiency": 0.3,
                    "epc": 98.3654595406082,
                },
            },
            "inverter": {
                "settings": {"is_selected": True, "design": True},
                "parameters": {
                    "nominal_capacity": None,
                    "lifetime": 25,
                    "capex": 415.0,
                    "opex": 9.0,
                    "efficiency": 0.95,
                    "epc": 63.01691128321419,
                },
            },
            "pv": {
                "settings": {"is_selected": True, "design": True},
                "parameters": {
                    "nominal_capacity": 441.0,
                    "lifetime": 25,
                    "capex": 441.0,
                    "opex": 8.8,
                    "epc": 66.20110331541557,
                },
            },
            "rectifier": {
                "settings": {"is_selected": True, "design": True},
                "parameters": {
                    "nominal_capacity": 5.0,
                    "lifetime": 25,
                    "capex": 415.0,
                    "opex": 0.0,
                    "efficiency": 0.95,
                    "epc": 54.0169112832142,
                },
            },
            "shortage": {
                "settings": {"is_selected": True},
                "parameters": {
                    "max_shortage_total": 10.0,
                    "max_shortage_timestep": 20.0,
                    "shortage_penalty_cost": 0.8,
                },
            },
        }
    )

    return supply.SupplyInput(sequences=sequences, energy_system_design=energy_system_design)


# There are a number of threads (workers). The following table shows what threads read/write/create
# what cells in the DB. The worker "exploration" creates all the other workers and waits for their
# finalization. The last three workers run in (P)arallel. We try to avoid parallel writings on the
# same column for the same row.
#
# parent                   | READ  | simulation (status)
#   (exploration and       | WRITE | exploration (clusters_found_at)
#   endpoints)             |       | exploration (optimizer_inputs_generated_at)
#                          |       | exploration (optimizer_finished_at)
#                          |       | exploration (status)
#                          |       | simulation (stopped_at)
# FindClusters             | READ  | exploration (status)
#                          | WRITE | cluster (all columns, creates)
#                          |       | exploration (clusters_found, minigrids_found)
# GenerateOptimizerInputs  | READ  | exploration (all columns)
#   (P)                    |       | cluster (all columns)
#                          | WRITE | simulation (all columns, creates)
# RunOptimizer             | READ  | exploration (minigrids_found)
#   (P)                    |       | simulation (all columns)
#                          | WRITE | simulation (optimizer_started_at, optimizer_failed_at)
#                          |       | simulation (grid_results, supply_results)
# ProcessSimulationResults | READ  | exploration (minigrids_found)
#   (P)                    |       | simulation (status, grid_results, supply_results)
#                          |       | cluster (all columns)
#                          | WRITE | cluster (many columns)
#                          |       | simulation (project_input)


class WorkerFindClusters:
    def __init__(
        self,
        parameters: ClusteringParametersCreate,
        exploration_id: pydantic.UUID4,
    ):
        self._parameters = parameters
        self._exploration_id = exploration_id
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerFindClusters") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration

            # Truncate table Cluster
            db_session.execute(sqlmodel.delete(Cluster))
            db_session.commit()

            db_clusters, discarded_clusters, outliers = generate_clusters(
                db_session, self._parameters
            )
            for c in db_clusters:
                if (
                    self._parameters.debug_consumer_count_max
                    and c.num_buildings > self._parameters.debug_consumer_count_max
                ):
                    continue

                db_session.add(c)

            db_session.commit()
            print("\n📁 Clustering results saved to DB.")

            db_exploration.clusters_found = (
                len(db_clusters) + len(discarded_clusters)  # + len(outliers)
            )
            db_exploration.minigrids_found = len(db_clusters)
            db_session.add(db_exploration)
            db_session.commit()

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


class WorkerGenerateOptimizerInputs:
    def __init__(self, exploration_id: pydantic.UUID4):
        self._exploration_id = exploration_id
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerGenerateOptimizerInputs") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration

            potential_minigrids = db_session.exec(sqlmodel.select(Cluster)).all()

            for minigrid in potential_minigrids:
                if self._stop_event.is_set():
                    self._result = None
                    return self._result

                grid_input, supply_input = self.generate_inputs(db_session, minigrid)

                db_simulation = Simulation(
                    exploration_id=self._exploration_id,
                    cluster_id=minigrid.cluster_id,
                    grid_input=grid_input.model_dump_json(),
                    supply_input=supply_input.model_dump_json(),
                    # status=None,
                )
                db_session.add(db_simulation)
                db_session.commit()
                db_session.refresh(db_simulation)

            self._result = None

    def generate_inputs(
        self, session: db.Session, cluster: Cluster
    ) -> tuple[grid.GridInput, supply.SupplyInput]:
        building_shp_ids = [building.building_id for building in cluster.buildings_as_objects]
        buildings = session.exec(
            sqlmodel.select(features.Building).where(
                sqlmodel.col(features.Building.id_shp).in_(building_shp_ids)
            )
        ).all()

        electric_demand = demand.calculate_demand(list(buildings), session)

        grid_input = generate_grid_input(electric_demand.total_annual_demand, cluster)

        supply_input = generate_supply_input(
            list(electric_demand.hourly_annual_demand.values()), cluster
        )

        return (grid_input, supply_input)

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


class WorkerRunOptimizer:
    def __init__(self, exploration_id: pydantic.UUID4):
        self._exploration_id = exploration_id
        self._finished = False
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerRunOptimizer") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration

            if not db_exploration.minigrids_found:
                print("No minigrids found in exploration, nothing to optimize.")
                return

            NUM_SLOTS: int = 8
            slots: list[
                tuple[
                    pydantic.UUID4 | None,
                    offgrid_planner.CheckerGrid | None,
                    offgrid_planner.CheckerSupply | None,
                ]
            ] = [(None, None, None) for _ in range(NUM_SLOTS)]

            stmt_pending_simulations = sqlmodel.select(Simulation).where(
                Simulation.exploration_id == self._exploration_id,
                Simulation.status == SimulationStatus.PENDING,
            )

            # Wait until some simulations are available in the DB
            while not db_session.exec(stmt_pending_simulations).first():
                if self._stop_event.is_set():
                    self._result = None
                    return self._result
                time.sleep(0.5)

            # Startup: fill up as many slots as possible with simulations
            executed_simulations: int = 0
            db_simulations = db_session.exec(stmt_pending_simulations.limit(NUM_SLOTS)).all()
            for i, db_simulation in enumerate(db_simulations):
                if self._stop_event.is_set():
                    self._result = None
                    return self._result

                # Wait some time to not choke the optimizer
                time.sleep(0.2)

                grid_input = grid.GridInput.model_validate_json(db_simulation.grid_input)
                supply_input = supply.SupplyInput.model_validate_json(db_simulation.supply_input)
                checker_grid = offgrid_planner.optimize_grid(grid_input)
                checker_supply = offgrid_planner.optimize_supply(supply_input)

                db_simulation.optimizer_started_at = datetime.datetime.now()
                db_session.add(db_simulation)
                db_session.commit()
                db_session.refresh(db_simulation)

                # Check errors. We either increase the number of executed simulations or fill up the
                # slot:
                if isinstance(checker_grid, offgrid_planner.ErrorServiceOffgridPlanner) or (
                    isinstance(checker_supply, offgrid_planner.ErrorServiceOffgridPlanner)
                ):
                    db_simulation.optimizer_failed_at = datetime.datetime.now()
                    db_session.add(db_simulation)
                    db_session.commit()
                    db_session.refresh(db_simulation)

                    executed_simulations += 1
                else:
                    slots[i] = (db_simulation.id, checker_grid, checker_supply)

            # Invariant: slots[] contains minigrids we have not finished running the simulation for,
            # yet. If the first value of the tuple is None, the other two are None as well, and it
            # means that the slot is empty and available for a new minigrid.

            assert db_exploration.minigrids_found, (
                "Exploration.minigrids_found not set by the clustering step"
            )

            while (
                not all(minigrid_id is None for minigrid_id, _g, _s in slots)
                or executed_simulations < db_exploration.minigrids_found
            ):
                for i, (minigrid_id, checker_grid, checker_supply) in enumerate(slots):
                    if self._stop_event.is_set():
                        self._result = None
                        return self._result

                    # If the slot is empty, fill it up with a remaining simulation
                    if not minigrid_id:
                        db_simulation = db_session.exec(stmt_pending_simulations).first()
                        if db_simulation:
                            grid_input = grid.GridInput.model_validate_json(
                                db_simulation.grid_input
                            )
                            supply_input = supply.SupplyInput.model_validate_json(
                                db_simulation.supply_input
                            )

                            # Wait some time to not choke the optimizer
                            time.sleep(0.2)

                            checker_grid = offgrid_planner.optimize_grid(grid_input)
                            checker_supply = offgrid_planner.optimize_supply(supply_input)

                            db_simulation.optimizer_started_at = datetime.datetime.now()
                            db_session.add(db_simulation)
                            db_session.commit()
                            db_session.refresh(db_simulation)

                            # Check errors. We either increase the number of executed simulations or
                            # fill up the slot:
                            if isinstance(
                                checker_grid, offgrid_planner.ErrorServiceOffgridPlanner
                            ) or isinstance(
                                checker_supply, offgrid_planner.ErrorServiceOffgridPlanner
                            ):
                                db_simulation.optimizer_failed_at = datetime.datetime.now()
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                                executed_simulations += 1
                            else:
                                slots[i] = (db_simulation.id, checker_grid, checker_supply)

                    # Slot not empty: check if either the grid or the supply optimizers have
                    # finished:
                    else:
                        if self._stop_event.is_set():
                            self._result = None
                            return self._result

                        db_simulation = db_session.get(Simulation, minigrid_id)

                        assert db_simulation

                        if checker_grid:
                            grid_output = checker_grid()

                            # Check errors
                            if isinstance(grid_output, offgrid_planner.ErrorServiceOffgridPlanner):
                                db_simulation.optimizer_failed_at = datetime.datetime.now()
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                                checker_grid = None
                                grid_output = None

                            elif (
                                grid_output
                                and grid_output.status == offgrid_planner.RequestStatus.DONE
                            ):
                                assert grid_output.results and not isinstance(
                                    grid_output.results, offgrid_planner.ErrorResultType
                                )

                                db_simulation.grid_results = grid_output.results.model_dump_json()
                                checker_grid = None
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                        if checker_supply:
                            supply_output = checker_supply()

                            # Check errors
                            if isinstance(
                                supply_output, offgrid_planner.ErrorServiceOffgridPlanner
                            ):
                                db_simulation.optimizer_failed_at = datetime.datetime.now()
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                                checker_supply = None
                                supply_output = None

                            elif (
                                supply_output
                                and supply_output.status == offgrid_planner.RequestStatus.DONE
                            ):
                                assert supply_output.results and not isinstance(
                                    supply_output.results, offgrid_planner.ErrorResultType
                                )

                                db_simulation.supply_results = (
                                    supply_output.results.model_dump_json()
                                )
                                checker_supply = None
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                        # Empty the slot if the simulation has finished:
                        if checker_grid is None and checker_supply is None:
                            minigrid_id = None
                            executed_simulations += 1
                            db_session.add(db_simulation)
                            db_session.commit()
                            db_session.refresh(db_simulation)

                        slots[i] = minigrid_id, checker_grid, checker_supply

                        time.sleep(0.5)

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


class WorkerProcessSimulationResults:
    def __init__(self, exploration_id: pydantic.UUID4):
        self._exploration_id = exploration_id
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerProcessSimulationResults") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration
            # assert db_exploration.minigrids_found
            if not db_exploration.minigrids_found:
                print("No minigrids found in exploration, nothing to process.")
                return

            stmt_ended_simulations_count = sqlmodel.select(sqlalchemy.func.count()).where(
                Simulation.exploration_id == self._exploration_id,
                Simulation.status.in_(  # type: ignore
                    [
                        SimulationStatus.ERROR,
                        SimulationStatus.STOPPED,
                        SimulationStatus.PROCESSED,
                    ]
                ),
            )

            while (
                db_session.scalar(stmt_ended_simulations_count) or 0
            ) < db_exploration.minigrids_found:
                db_simulations = db_session.exec(
                    sqlmodel.select(Simulation).where(
                        Simulation.exploration_id == self._exploration_id,
                        Simulation.status == SimulationStatus.OPTIMIZED,
                    )
                ).all()
                if self._stop_event.is_set():
                    self._result = None
                    return self._result

                for simulation in db_simulations:
                    if self._stop_event.is_set():
                        self._result = None
                        return self._result

                    if simulation.status == SimulationStatus.OPTIMIZED:
                        sim_results = self.process_simulation_results(simulation)
                        cluster = db_session.exec(
                            sqlmodel.select(Cluster).where(
                                Cluster.cluster_id == simulation.cluster_id
                            )
                        ).one()
                        assert cluster
                        updates = sim_results.model_dump(exclude_none=True)
                        for field, val in updates.items():
                            setattr(cluster, field, val)

                        db_session.add(cluster)
                        db_session.add(simulation)
                        db_session.commit()

                time.sleep(0.5)

    def _project_inputs(self) -> dict[str, typing.Any]:
        ProjectKeys: list[str] = ["n_days", "interest_rate", "tax", "lifetime"]
        ProjectJson: dict[str, typing.Any] = {}
        for key in ProjectKeys:
            value = getattr(self.project, key)
            ProjectJson.update({key: value})

        return ProjectJson

    def process_simulation_results(self, simulation: Simulation) -> project_result.ResultsSummary:
        self.project = project_result.Project(id=simulation.id)
        grid_input = grid.GridInput.model_validate_json(simulation.grid_input)
        self.project.grid_inputs = grid_input
        self.project.load_grid_inputs()

        supply_input = supply.SupplyInput.model_validate_json(simulation.supply_input)
        self.project.supply_inputs = supply_input
        self.project.load_supply_inputs()

        if simulation.grid_results:
            grid_output = grid.GridResult.model_validate_json(simulation.grid_results)
            self.project.grid_outputs = grid_output

        if simulation.supply_results:
            supply_output = supply.SupplyResult.model_validate_json(simulation.supply_results)
            self.project.supply_outputs = supply_output

        self.project.grid_results()
        self.project.supply_results()

        simulation.project_input = json.dumps(self._project_inputs())

        return self.project.get_results_summary()

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


# Module level variable with a lock for thread safety:
active_workers_lock = threading.Lock()
active_workers: dict[
    str,
    WorkerFindClusters
    | WorkerGenerateOptimizerInputs
    | WorkerRunOptimizer
    | WorkerProcessSimulationResults,
] = {}


def register_worker(
    name: str,
    worker: WorkerFindClusters
    | WorkerGenerateOptimizerInputs
    | WorkerRunOptimizer
    | WorkerProcessSimulationResults,
):
    with active_workers_lock:
        active_workers[name] = worker


def get_worker(name: str):
    with active_workers_lock:
        return active_workers.get(name)


def remove_worker(name: str):
    with active_workers_lock:
        active_workers.pop(name, None)


def worker_exploration(parameters: ClusteringParametersCreate, exploration_id: pydantic.UUID4):
    """Worker to be used as the target of a thread. It creates 4 sub-threads and waits until all of
    them finish."""

    with db.get_logging_session("worker exploration") as db_session:
        worker_clusters = WorkerFindClusters(parameters, exploration_id)
        thread_clusters = threading.Thread(target=worker_clusters)
        register_worker(f"clusters/{exploration_id}", worker_clusters)
        thread_clusters.start()
        thread_clusters.join()

        db_exploration = db_session.get(Exploration, exploration_id)

        assert db_exploration

        db_exploration.clusters_found_at = datetime.datetime.now()
        db_session.add(db_exploration)
        db_session.commit()

        # The following 3 workers can run in parallel
        worker_inputs = WorkerGenerateOptimizerInputs(exploration_id)
        thread_inputs = threading.Thread(target=worker_inputs)
        register_worker(f"inputs/{exploration_id}", worker_inputs)
        thread_inputs.start()

        worker_optimizer = WorkerRunOptimizer(exploration_id)
        thread_optimizer = threading.Thread(target=worker_optimizer)
        register_worker(f"optimizer/{exploration_id}", worker_optimizer)
        thread_optimizer.start()

        worker_results = WorkerProcessSimulationResults(exploration_id)
        thread_results = threading.Thread(target=worker_results)
        register_worker(f"results/{exploration_id}", worker_results)
        thread_results.start()

        thread_inputs.join()
        db_exploration.optimizer_inputs_generated_at = datetime.datetime.now()
        db_session.add(db_exploration)
        db_session.commit()

        thread_optimizer.join()
        db_exploration.optimizer_finished_at = datetime.datetime.now()
        db_session.add(db_exploration)
        db_session.commit()

        thread_results.join()
        db_exploration.status = ExplorationStatus.FINISHED
        db_session.add(db_exploration)
        db_session.commit()


def start_exploration(
    db: db.Session, parameters: ClusteringParameters
) -> pydantic.UUID4 | ExplorationError:
    db_exploration = Exploration.model_validate(parameters)
    db_exploration.status = ExplorationStatus.RUNNING
    db.add(db_exploration)
    db.commit()
    db.refresh(db_exploration)

    if db_exploration.status != ExplorationStatus.STOPPED:  # type: ignore
        thread = threading.Thread(
            target=worker_exploration,
            args=(parameters, db_exploration.id),
            name=f"exploration/{db_exploration.id}",
        )
        thread.start()

    # We don't wait until the thread finishes, we want to return asap
    return db_exploration.id


def stop_exploration(db: db.Session, exploration_id: pydantic.UUID4):
    db_exploration = db.get(Exploration, exploration_id)

    assert db_exploration

    db_exploration.status = ExplorationStatus.STOPPED

    db_exploration_simulations = db.exec(
        sqlmodel.select(Simulation).where(
            sqlmodel.and_(
                Simulation.exploration_id == exploration_id,
            )
        )
    ).all()

    for db_simulation in db_exploration_simulations:
        db_simulation.stopped_at = datetime.datetime.now()

    db.commit()

    worker_clusters = get_worker(f"clusters/{exploration_id}")
    if worker_clusters:
        worker_clusters.stop()
        remove_worker(f"clusters/{exploration_id}")

    worker_inputs = get_worker(f"inputs/{exploration_id}")
    if worker_inputs:
        worker_inputs.stop()
        remove_worker(f"inputs/{exploration_id}")

    worker_optimizer = get_worker(f"optimizer/{exploration_id}")
    if worker_optimizer:
        worker_optimizer.stop()
        remove_worker(f"optimizer/{exploration_id}")

    worker_results = get_worker(f"results/{exploration_id}")
    if worker_results:
        worker_results.stop()
        remove_worker(f"results/{exploration_id}")


if __name__ == "__main__":
    import app.db.core as db

    parameters = ClusteringParameters()

    with sqlmodel.Session(db.get_engine()) as session:
        start_exploration(session, parameters)
