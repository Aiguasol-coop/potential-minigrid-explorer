import datetime
import enum
import threading
import time
import json
import uuid
import typing

import pydantic
import sqlalchemy
import sqlalchemy.types
import sqlmodel

import app.db.core as db
import app.service_offgrid_planner.grid as grid
import app.service_offgrid_planner.service as offgrid_planner
import app.service_offgrid_planner.supply as supply
import app.service_offgrid_planner.results as project_result
from app.explorations.clustering import Cluster, ClusteringParameters, generate_clusters


class ExplorationStatus(str, enum.Enum):
    """A FINISHED exploration can contain failed simulations (even all of them could have failed).
    You have to check the status of the simulations to know."""

    RUNNING = "RUNNING"
    FINISHED = "FINISHED"
    STOPPED = "STOPPED"


class Exploration(ClusteringParameters, table=True):
    id: pydantic.UUID4 = sqlmodel.Field(default_factory=uuid.uuid4, primary_key=True)

    status: ExplorationStatus = sqlmodel.Field(
        default=ExplorationStatus.RUNNING,
        sa_column=sqlalchemy.Column(sqlalchemy.Enum(ExplorationStatus), nullable=False),
    )

    minigrids_found: int | None = None

    clusters_found: int | None = None

    clusters_found_at: datetime.datetime | None = None

    optimizer_inputs_generated_at: datetime.datetime | None = None

    optimizer_finished_at: datetime.datetime | None = None

    created_at: datetime.datetime = sqlmodel.Field(default_factory=lambda: datetime.datetime.now())


class ProjectStatus(enum.Enum):
    POTENTIAL = "POTENTIAL"
    PROJECT = "PROJECT"
    MONITORING = "MONITORING"


class SimulationStatus(str, enum.Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    OPTIMIZED = "OPTIMIZED"
    PROCESSED = "PROCESSED"
    STOPPED = "STOPPED"
    ERROR = "ERROR"


class SimulationStatusSql(sqlalchemy.types.TypeDecorator[str]):
    impl = sqlalchemy.String
    cache_ok = True

    def process_bind_param(
        self, value: SimulationStatus | str | None, dialect: sqlalchemy.engine.interfaces.Dialect
    ) -> str | None:
        if isinstance(value, SimulationStatus):
            return value.value
        return value

    def process_result_value(
        self, value: str | None, dialect: sqlalchemy.engine.interfaces.Dialect
    ) -> SimulationStatus | None:
        if value is not None:
            return SimulationStatus(value)
        return value


class Simulation(sqlmodel.SQLModel, table=True):
    id: pydantic.UUID4 = sqlmodel.Field(default_factory=uuid.uuid4, primary_key=True)

    exploration_id: pydantic.UUID4 = sqlmodel.Field(foreign_key="exploration.id")

    cluster_id: int
    """External identifier generated by the clustering algorithm."""

    # WARNING: Be careful when updating the following attributes, read
    # https://amercader.net/blog/beware-of-json-fields-in-sqlalchemy/. Recommended action: use deep
    # copy (probably from pydantic), as explained at the end of the article.

    project_input: str | None = sqlmodel.Field(
        sa_column=sqlalchemy.Column(sqlalchemy.Text), default=None
    )

    grid_input: str = sqlmodel.Field(sa_column=sqlalchemy.Column(sqlalchemy.Text))

    grid_results: str | None = sqlmodel.Field(
        sa_column=sqlalchemy.Column(sqlalchemy.Text), default=None
    )

    supply_input: str = sqlmodel.Field(sa_column=sqlalchemy.Column(sqlalchemy.Text))

    supply_results: str | None = sqlmodel.Field(
        sa_column=sqlalchemy.Column(sqlalchemy.Text), default=None
    )

    optimizer_started_at: datetime.datetime | None = None

    optimizer_failed_at: datetime.datetime | None = None

    stopped_at: datetime.datetime | None = None

    created_at: datetime.datetime = sqlmodel.Field(default_factory=lambda: datetime.datetime.now())

    status: SimulationStatus = sqlmodel.Field(
        default=None,
        sa_column=sqlalchemy.Column(
            SimulationStatusSql(),
            sqlalchemy.Computed(
                """
                CASE
                    WHEN optimizer_failed_at IS NOT NULL THEN 'ERROR'
                    WHEN project_input IS NOT NULL THEN 'PROCESSED'
                    WHEN stopped_at IS NOT NULL THEN 'STOPPED'
                    WHEN grid_results IS NOT NULL AND supply_results IS NOT NULL THEN 'OPTIMIZED'
                    WHEN optimizer_started_at IS NOT NULL THEN 'RUNNING'
                    ELSE 'PENDING'
                END
                """,
                persisted=True,
            ),
            nullable=False,
        ),
    )

    __table_args__ = (
        sqlalchemy.UniqueConstraint(
            "exploration_id", "cluster_id", name="uc_exploration_id_cluster_id"
        ),
    )


class ExplorationError(str, enum.Enum):
    start_clustering_failed = "The clustering algorithm could not be launched"
    clustering_algorithm_failed = "The clustering algorithm failed"


class CategoryDistribution(sqlmodel.SQLModel, table=True):
    """Store the distribution of different categories in the centroids."""

    id: int = sqlmodel.Field(primary_key=True, default=None)
    households: float
    public_services: float
    enterprises: float
    created_at: datetime.datetime = sqlmodel.Field(default_factory=datetime.datetime.now)


# There are a number of threads (workers). The following table shows what threads read/write/create
# what cells in the DB. The worker "exploration" creates all the other workers and waits for their
# finalization. The last three workers run in (P)arallel. We try to avoid parallel writings on the
# same column for the same row.
#
# parent                   | READ  | simulation (status)
#   (exploration and       | WRITE | exploration (clusters_found_at)
#   endpoints)             |       | exploration (optimizer_inputs_generated_at)
#                          |       | exploration (optimizer_finished_at)
#                          |       | exploration (status)
#                          |       | simulation (stopped_at)
# FindClusters             | READ  | exploration (status)
#                          | WRITE | cluster (all columns, creates)
#                          |       | exploration (clusters_found, minigrids_found)
# GenerateOptimizerInputs  | READ  | exploration (all columns)
#   (P)                    |       | cluster (all columns)
#                          | WRITE | simulation (all columns, creates)
# RunOptimizer             | READ  | exploration (minigrids_found)
#   (P)                    |       | simulation (all columns)
#                          | WRITE | simulation (optimizer_started_at, optimizer_failed_at)
#                          |       | simulation (grid_results, supply_results)
# ProcessSimulationResults | READ  | exploration (minigrids_found)
#   (P)                    |       | simulation (status, grid_results, supply_results)
#                          |       | cluster (all columns)
#                          | WRITE | cluster (many columns)
#                          |       | simulation (project_input)


class WorkerFindClusters:
    def __init__(self, parameters: ClusteringParameters, exploration_id: pydantic.UUID4):
        self._parameters = parameters
        self._exploration_id = exploration_id
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerFindClusters") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration

            # Truncate table Cluster
            db_session.execute(sqlmodel.delete(Cluster))
            db_session.commit()

            db_clusters, discarded_clusters, outliers = generate_clusters(
                db_session, self._parameters
            )
            for c in db_clusters:
                db_session.add(c)

            db_session.commit()
            print("\nðŸ“ Clustering results saved to DB.")

            db_exploration.clusters_found = (
                len(db_clusters) + len(discarded_clusters) + len(outliers)
            )
            db_exploration.minigrids_found = len(db_clusters)
            db_session.add(db_exploration)
            db_session.commit()

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


class WorkerGenerateOptimizerInputs:
    def __init__(self, exploration_id: pydantic.UUID4):
        self._exploration_id = exploration_id
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerGenerateOptimizerInputs") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration

            potential_minigrids = db_session.exec(sqlmodel.select(Cluster)).all()

            for minigrid in potential_minigrids:
                if self._stop_event.is_set():
                    self._result = None
                    return self._result

                grid_input, supply_input = self.generate_inputs(minigrid)

                db_simulation = Simulation(
                    exploration_id=self._exploration_id,
                    cluster_id=minigrid.cluster_id,
                    grid_input=grid_input.model_dump_json(),
                    supply_input=supply_input.model_dump_json(),
                    # status=None,
                )
                db_session.add(db_simulation)
                db_session.commit()
                db_session.refresh(db_simulation)

            self._result = None

    def generate_inputs(self, cluster: Cluster) -> tuple[grid.GridInput, supply.SupplyInput]:
        # TODO: codi del Michel aquÃ­
        #
        ### BEGIN of FAKE code

        import pathlib

        input_json = pathlib.Path("src/tests/examples/grid_input_example.json").read_text()
        grid_input = grid.GridInput.model_validate_json(input_json)
        input_json = pathlib.Path("src/tests/examples/supply_input_example.json").read_text()
        supply_input = supply.SupplyInput.model_validate_json(input_json)

        ### END of FAKE code

        return (grid_input, supply_input)

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


class WorkerRunOptimizer:
    def __init__(self, exploration_id: pydantic.UUID4):
        self._exploration_id = exploration_id
        self._finished = False
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerRunOptimizer") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration

            NUM_SLOTS: int = 8
            slots: list[
                tuple[
                    pydantic.UUID4 | None,
                    offgrid_planner.CheckerGrid | None,
                    offgrid_planner.CheckerSupply | None,
                ]
            ] = [(None, None, None) for _ in range(NUM_SLOTS)]

            stmt_pending_simulations = sqlmodel.select(Simulation).where(
                Simulation.exploration_id == self._exploration_id,
                Simulation.status == SimulationStatus.PENDING,
            )

            # Wait until some simulations are available in the DB
            while not db_session.exec(stmt_pending_simulations).first():
                if self._stop_event.is_set():
                    self._result = None
                    return self._result
                time.sleep(0.5)

            # Startup: fill up as many slots as possible with simulations
            executed_simulations: int = 0
            db_simulations = db_session.exec(stmt_pending_simulations.limit(NUM_SLOTS)).all()
            for i, db_simulation in enumerate(db_simulations):
                if self._stop_event.is_set():
                    self._result = None
                    return self._result

                # Wait some time to not choke the optimizer
                time.sleep(0.2)

                grid_input = grid.GridInput.model_validate(json.loads(db_simulation.grid_input))
                supply_input = supply.SupplyInput.model_validate(
                    json.loads(db_simulation.supply_input)
                )
                checker_grid = offgrid_planner.optimize_grid(grid_input)
                checker_supply = offgrid_planner.optimize_supply(supply_input)

                db_simulation.optimizer_started_at = datetime.datetime.now()
                db_session.add(db_simulation)
                db_session.commit()
                db_session.refresh(db_simulation)

                # Check errors. We either increase the number of executed simulations or fill up the
                # slot:
                if isinstance(checker_grid, offgrid_planner.ErrorServiceOffgridPlanner) or (
                    isinstance(checker_supply, offgrid_planner.ErrorServiceOffgridPlanner)
                ):
                    db_simulation.optimizer_failed_at = datetime.datetime.now()
                    db_session.add(db_simulation)
                    db_session.commit()
                    db_session.refresh(db_simulation)

                    executed_simulations += 1
                else:
                    slots[i] = (db_simulation.id, checker_grid, checker_supply)

            # Invariant: slots[] contains minigrids we have not finished running the simulation for,
            # yet. If the first value of the tuple is None, the other two are None as well, and it
            # means that the slot is empty and available for a new minigrid.

            assert db_exploration.minigrids_found, (
                "Exploration.minigrids_found not set by the clustering step"
            )

            while (
                not all(minigrid_id is None for minigrid_id, _g, _s in slots)
                or executed_simulations < db_exploration.minigrids_found
            ):
                for i, (minigrid_id, checker_grid, checker_supply) in enumerate(slots):
                    if self._stop_event.is_set():
                        self._result = None
                        return self._result

                    # If the slot is empty, fill it up with a remaining simulation
                    if not minigrid_id:
                        db_simulation = db_session.exec(stmt_pending_simulations).first()
                        if db_simulation:
                            grid_input = grid.GridInput.model_validate(
                                json.loads(db_simulation.grid_input)
                            )
                            supply_input = supply.SupplyInput.model_validate(
                                json.loads(db_simulation.supply_input)
                            )

                            # Wait some time to not choke the optimizer
                            time.sleep(0.2)

                            checker_grid = offgrid_planner.optimize_grid(grid_input)
                            checker_supply = offgrid_planner.optimize_supply(supply_input)

                            db_simulation.optimizer_started_at = datetime.datetime.now()
                            db_session.add(db_simulation)
                            db_session.commit()
                            db_session.refresh(db_simulation)

                            # Check errors. We either increase the number of executed simulations or
                            # fill up the slot:
                            if isinstance(
                                checker_grid, offgrid_planner.ErrorServiceOffgridPlanner
                            ) or isinstance(
                                checker_supply, offgrid_planner.ErrorServiceOffgridPlanner
                            ):
                                db_simulation.optimizer_failed_at = datetime.datetime.now()
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                                executed_simulations += 1
                            else:
                                slots[i] = (db_simulation.id, checker_grid, checker_supply)

                    # Slot not empty: check if either the grid or the supply optimizers have
                    # finished:
                    else:
                        if self._stop_event.is_set():
                            self._result = None
                            return self._result

                        db_simulation = db_session.get(Simulation, minigrid_id)

                        assert db_simulation

                        if checker_grid:
                            grid_output = checker_grid()

                            # Check errors
                            if isinstance(grid_output, offgrid_planner.ErrorServiceOffgridPlanner):
                                db_simulation.optimizer_failed_at = datetime.datetime.now()
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                                checker_grid = None
                                grid_output = None

                            elif (
                                grid_output
                                and grid_output.status == offgrid_planner.RequestStatus.DONE
                            ):
                                assert grid_output.results and not isinstance(
                                    grid_output.results, offgrid_planner.ErrorResultType
                                )

                                db_simulation.grid_results = grid_output.results.model_dump_json()
                                checker_grid = None
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                        if checker_supply:
                            supply_output = checker_supply()

                            # Check errors
                            if isinstance(
                                supply_output, offgrid_planner.ErrorServiceOffgridPlanner
                            ):
                                db_simulation.optimizer_failed_at = datetime.datetime.now()
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                                checker_supply = None
                                supply_output = None

                            elif (
                                supply_output
                                and supply_output.status == offgrid_planner.RequestStatus.DONE
                            ):
                                assert supply_output.results and not isinstance(
                                    supply_output.results, offgrid_planner.ErrorResultType
                                )

                                db_simulation.supply_results = (
                                    supply_output.results.model_dump_json()
                                )
                                checker_supply = None
                                db_session.add(db_simulation)
                                db_session.commit()
                                db_session.refresh(db_simulation)

                        # Empty the slot if the simulation has finished:
                        if checker_grid is None and checker_supply is None:
                            minigrid_id = None
                            executed_simulations += 1
                            db_session.add(db_simulation)
                            db_session.commit()
                            db_session.refresh(db_simulation)

                        slots[i] = minigrid_id, checker_grid, checker_supply

                        time.sleep(0.5)

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


class WorkerProcessSimulationResults:
    def __init__(self, exploration_id: pydantic.UUID4):
        self._exploration_id = exploration_id
        self._result: None | ExplorationError = None
        self._stop_event = threading.Event()

    def __call__(self) -> None:
        with db.get_logging_session("WorkerProcessSimulationResults") as db_session:
            db_exploration = db_session.get(Exploration, self._exploration_id)

            assert db_exploration
            assert db_exploration.minigrids_found

            stmt_ended_simulations_count = sqlmodel.select(sqlalchemy.func.count()).where(
                Simulation.exploration_id == self._exploration_id,
                Simulation.status.in_(  # type: ignore
                    [
                        SimulationStatus.ERROR,
                        SimulationStatus.STOPPED,
                        SimulationStatus.PROCESSED,
                    ]
                ),
            )

            while (
                db_session.scalar(stmt_ended_simulations_count) or 0
            ) < db_exploration.minigrids_found:
                db_simulations = db_session.exec(
                    sqlmodel.select(Simulation).where(
                        Simulation.exploration_id == self._exploration_id,
                        Simulation.status == SimulationStatus.OPTIMIZED,
                    )
                ).all()
                if self._stop_event.is_set():
                    self._result = None
                    return self._result

                for simulation in db_simulations:
                    if self._stop_event.is_set():
                        self._result = None
                        return self._result

                    if simulation.status == SimulationStatus.OPTIMIZED:
                        sim_results = self.process_simulation_results(simulation)
                        cluster = db_session.exec(
                            sqlmodel.select(Cluster).where(
                                Cluster.cluster_id == simulation.cluster_id
                            )
                        ).one()
                        assert cluster
                        updates = sim_results.model_dump(exclude_none=True)
                        for field, val in updates.items():
                            setattr(cluster, field, val)

                        db_session.add(cluster)
                        db_session.add(simulation)
                        db_session.commit()

                time.sleep(0.5)

    def _project_inputs(self) -> dict[str, typing.Any]:
        ProjectKeys: list[str] = ["n_days", "interest_rate", "tax", "lifetime"]
        ProjectJson: dict[str, typing.Any] = {}
        for key in ProjectKeys:
            value = getattr(self.project, key)
            ProjectJson.update({key: value})

        return ProjectJson

    def process_simulation_results(self, simulation: Simulation) -> project_result.ResultsSummary:
        self.project = project_result.Project(id=simulation.id)
        grid_input = grid.GridInput.model_validate_json(simulation.grid_input)
        self.project.grid_inputs = grid_input
        self.project.load_grid_inputs()

        supply_input = supply.SupplyInput.model_validate_json(simulation.supply_input)
        self.project.supply_inputs = supply_input
        self.project.load_supply_inputs()

        if simulation.grid_results:
            grid_output = grid.GridResult.model_validate_json(simulation.grid_results)
            self.project.grid_outputs = grid_output

        if simulation.supply_results:
            supply_output = supply.SupplyResult.model_validate_json(simulation.supply_results)
            self.project.supply_outputs = supply_output

        self.project.grid_results()
        self.project.supply_results()

        simulation.project_input = json.dumps(self._project_inputs())

        return self.project.get_results_summary()

    def stop(self):
        self._stop_event.set()

    @property
    def result(self) -> None | ExplorationError:
        return self._result


# Module level variable with a lock for thread safety:
active_workers_lock = threading.Lock()
active_workers: dict[
    str,
    WorkerFindClusters
    | WorkerGenerateOptimizerInputs
    | WorkerRunOptimizer
    | WorkerProcessSimulationResults,
] = {}


def register_worker(
    name: str,
    worker: WorkerFindClusters
    | WorkerGenerateOptimizerInputs
    | WorkerRunOptimizer
    | WorkerProcessSimulationResults,
):
    with active_workers_lock:
        active_workers[name] = worker


def get_worker(name: str):
    with active_workers_lock:
        return active_workers.get(name)


def remove_worker(name: str):
    with active_workers_lock:
        active_workers.pop(name, None)


def worker_exploration(parameters: ClusteringParameters, exploration_id: pydantic.UUID4):
    """Worker to be used as the target of a thread. It creates 4 sub-threads and waits until all of
    them finish."""

    with db.get_logging_session("worker exploration") as db_session:
        worker_clusters = WorkerFindClusters(parameters, exploration_id)
        thread_clusters = threading.Thread(target=worker_clusters)
        register_worker(f"clusters/{exploration_id}", worker_clusters)
        thread_clusters.start()
        thread_clusters.join()

        db_exploration = db_session.get(Exploration, exploration_id)

        assert db_exploration

        db_exploration.clusters_found_at = datetime.datetime.now()
        db_session.add(db_exploration)
        db_session.commit()

        # The following 3 workers can run in parallel
        worker_inputs = WorkerGenerateOptimizerInputs(exploration_id)
        thread_inputs = threading.Thread(target=worker_inputs)
        register_worker(f"inputs/{exploration_id}", worker_inputs)
        thread_inputs.start()

        worker_optimizer = WorkerRunOptimizer(exploration_id)
        thread_optimizer = threading.Thread(target=worker_optimizer)
        register_worker(f"optimizer/{exploration_id}", worker_optimizer)
        thread_optimizer.start()

        worker_results = WorkerProcessSimulationResults(exploration_id)
        thread_results = threading.Thread(target=worker_results)
        register_worker(f"results/{exploration_id}", worker_results)
        thread_results.start()

        thread_inputs.join()
        db_exploration.optimizer_inputs_generated_at = datetime.datetime.now()
        db_session.add(db_exploration)
        db_session.commit()

        thread_optimizer.join()
        db_exploration.optimizer_finished_at = datetime.datetime.now()
        db_session.add(db_exploration)
        db_session.commit()

        thread_results.join()
        db_exploration.status = ExplorationStatus.FINISHED
        db_session.add(db_exploration)
        db_session.commit()


def start_exploration(
    db: db.Session, parameters: ClusteringParameters
) -> pydantic.UUID4 | ExplorationError:
    db_exploration = Exploration.model_validate(parameters)
    db_exploration.status = ExplorationStatus.RUNNING
    db.add(db_exploration)
    db.commit()
    db.refresh(db_exploration)

    if db_exploration.status != ExplorationStatus.STOPPED:  # type: ignore
        thread = threading.Thread(
            target=worker_exploration,
            args=(parameters, db_exploration.id),
            name=f"exploration/{db_exploration.id}",
        )
        thread.start()

    # We don't wait until the thread finishes, we want to return asap
    return db_exploration.id


def stop_exploration(db: db.Session, exploration_id: pydantic.UUID4):
    db_exploration = db.get(Exploration, exploration_id)

    assert db_exploration

    db_exploration.status = ExplorationStatus.STOPPED

    db_exploration_simulations = db.exec(
        sqlmodel.select(Simulation).where(
            sqlmodel.and_(
                Simulation.exploration_id == exploration_id,
            )
        )
    ).all()

    for db_simulation in db_exploration_simulations:
        db_simulation.stopped_at = datetime.datetime.now()

    db.commit()

    worker_clusters = get_worker(f"clusters/{exploration_id}")
    if worker_clusters:
        worker_clusters.stop()
        remove_worker(f"clusters/{exploration_id}")

    worker_inputs = get_worker(f"inputs/{exploration_id}")
    if worker_inputs:
        worker_inputs.stop()
        remove_worker(f"inputs/{exploration_id}")

    worker_optimizer = get_worker(f"optimizer/{exploration_id}")
    if worker_optimizer:
        worker_optimizer.stop()
        remove_worker(f"optimizer/{exploration_id}")

    worker_results = get_worker(f"results/{exploration_id}")
    if worker_results:
        worker_results.stop()
        remove_worker(f"results/{exploration_id}")


if __name__ == "__main__":
    import app.db.core as db

    parameters = ClusteringParameters()

    with sqlmodel.Session(db.get_engine()) as session:
        start_exploration(session, parameters)
